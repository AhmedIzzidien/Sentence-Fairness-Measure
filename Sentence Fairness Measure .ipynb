{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248141bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper Title: Developing a Sentence Level Fairness Metric Using Word Embeddings\n",
    "# Coded by Ahmed Izzidien\n",
    "\n",
    "# Install the following when running for first time \n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install seaborn \n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install pyinflect\n",
    "!{sys.executable} -m pip install lemminflect\n",
    "!{sys.executable} -m pip install graphviz\n",
    "!{sys.executable} -m pip install pydotplus\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install tensorflow #(or install using anaconda if denied access: https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/ )\n",
    "!{sys.executable} -m pip install -U pytorch-pretrained-bert\n",
    "!pip install https://github.com/MartinoMensio/spacy-universal-sentence-encoder/releases/download/v0.4.3/en_use_md-0.4.3.tar.gz \n",
    "!pip install fitbert\n",
    "!pip install wonderwords\n",
    "!pip install pattern\n",
    "!pip install spacy-universal-sentence-encoder\n",
    "!{sys.executable} -m pip install plotly==5.3.1\n",
    "!pip install spacy\n",
    "!pip U install sentence-transformers\n",
    "!pip install transformers\n",
    "!pip install flair -U\n",
    "!pip install -U textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemminflect \n",
    "import gzip\n",
    "import transformers\n",
    "transformers.__version__\n",
    "from transformers import pipeline\n",
    "import pyinflect\n",
    "import wonderwords\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import docx\n",
    "import re\n",
    "import seaborn as sns\n",
    "import nltk \n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import wordnet \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import re\n",
    "import sympy\n",
    "import torch\n",
    "torch.cuda.is_available()  #(if not available: it may be that there is no GPU in the computer)\n",
    "import seaborn as sns\n",
    "from fitbert import FitBert\n",
    "fb = FitBert()\n",
    "import random\n",
    "from wonderwords import RandomWord\n",
    "from wonderwords import RandomSentence\n",
    "s = RandomSentence()\n",
    "r = RandomWord()\n",
    "unmasker = pipeline('fill-mask', model='albert-xxlarge-v2')\n",
    "#Load the Universal Sentence Encoder's TF Hub module\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "  return model(input)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from transformers import pipeline\n",
    "#Load the Universal Sentence Encoder's TF Hub module\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data that will be tested with the code e.g. test sentences \n",
    "\n",
    "from fair_unfair import fair_unfair\n",
    "from thirty_six_fair_unfair import thirty_six_fair_unfair \n",
    "\n",
    "pd.options.display.max_rows = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe10bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Sentence FairVec (S-FairVec) using abstract concepts from the psych. litrature to represent the social ontology of fairness \n",
    " \n",
    "res = \"is was very responsible\"\n",
    "resA = [res]\n",
    "resV = embed(resA)\n",
    "\n",
    "irres = \"is was very irresponsible\"\n",
    "irresA = [irres]\n",
    "irresV = embed(irresA)\n",
    "\n",
    "joy = \"it was joyous\"\n",
    "joyA = [joy]\n",
    "joyV = embed(joyA)\n",
    "\n",
    "pain = \"it was sad\"\n",
    "painA = [pain]\n",
    "painV = embed(painA)\n",
    "\n",
    "ben = \"it was beneficial to society\"\n",
    "benA = [ben]\n",
    "benV = embed(benA)\n",
    "\n",
    "har = \"it was not beneficial to society\"\n",
    "harA = [har]\n",
    "harV = embed(harA)\n",
    "\n",
    "#lib = \"liberty\"\n",
    "lib = \"was free to and rewarded\"\n",
    "libA = [lib]\n",
    "libV = embed(libA)\n",
    "\n",
    "pri = \"was sent to prison and punished\"\n",
    "priA = [pri]\n",
    "priV = embed(priA)\n",
    "\n",
    "app = \"it was beneficial\"\n",
    "appA = [app]\n",
    "appV = embed(appA)\n",
    "\n",
    "inapp = \"it was harmful\"\n",
    "inappA = [inapp]\n",
    "inappV = embed(inappA)\n",
    "\n",
    "\n",
    "ResponsibilityV = resV - irresV \n",
    "EmotionV = joyV - painV \n",
    "ConsequenceV = libV - priV   \n",
    "BeneficialV = benV - harV \n",
    "HarmV = appV - inappV  \n",
    "\n",
    "#This is S-FairVec which combines the above vectors into one (it needs error minimisation)\n",
    "SFairVec = resV - irresV + joyV - painV  + libV - priV   + benV - harV  + appV - inappV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c71f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compare S-FairVec agaist a list of the randomly selected 36 sentences\n",
    "\n",
    "fair_s = \"it was fair\"\n",
    "fair_sA = [fair_s]\n",
    "fair_sV = embed(fair_sA)\n",
    "\n",
    "unfair_s = \"it was unfair\"\n",
    "unfair_sA = [unfair_s]\n",
    "unfair_sV = embed(unfair_sA)\n",
    "\n",
    "word_fair_unfair= fair_sV-unfair_sV\n",
    "\n",
    "def run_word_fairVec(sentences_to_test):\n",
    "    word_FairVecResults=[]\n",
    "    lenght = len(sentences_to_test)\n",
    "    sentence_embeddings = embed(sentences_to_test)\n",
    "    for i in range(lenght):\n",
    "        dot =  np.inner(word_fair_unfair,sentence_embeddings[i])\n",
    "        word_FairVecResults.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':sentences_to_test[i],\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "    word_df_fairness=pd.DataFrame(word_FairVecResults)\n",
    "    word_df_fairness['Score'] = word_df_fairness['Score'].astype(float)\n",
    "    word_df_fairness['Sentence'] = word_df_fairness['Sentence'].astype(str)\n",
    "    return word_df_fairness \n",
    "\n",
    "\n",
    "\n",
    "#Carry out word_FairVec\n",
    "word_fair_results=run_word_fairVec(thirty_six_fair_unfair)\n",
    "\n",
    "\n",
    "ax_word_fair = word_fair_results.plot.bar(x='Sentence', y='Score', legend=False, figsize=(14,14))\n",
    "ax_word_fair.tick_params(labelbottom=True,labeltop=False)\n",
    "plt.xticks(rotation = 90)\n",
    "ax_word_fair.axvline(17.5, color='k', linestyle='--')\n",
    "ax_word_fair.set_facecolor(\"white\")\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.grid(color='0.3', linestyle='-', linewidth=0.1)\n",
    "plt.ylabel('Cosine Similarity', fontsize=16)\n",
    "plt.xlabel('Sentences', fontsize=16)\n",
    "plt.ylim(-0.1, 0.1)\n",
    "plt.grid(True, which='both')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d01ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compare S-FairVec agaist the full list 200 of sentences\n",
    "\n",
    "fair_s = \"it was fair\"\n",
    "fair_sA = [fair_s]\n",
    "fair_sV = embed(fair_sA)\n",
    "\n",
    "unfair_s = \"it was not fair\"\n",
    "unfair_sA = [unfair_s]\n",
    "unfair_sV = embed(unfair_sA)\n",
    "\n",
    "word_fair_unfair= fair_sV-unfair_sV\n",
    "\n",
    "def run_word_fairVec(sentences_to_test):\n",
    "    word_FairVecResults=[]\n",
    "    lenght = len(sentences_to_test)\n",
    "    sentence_embeddings = embed(sentences_to_test)\n",
    "   \n",
    "    for i in range(lenght):\n",
    "        dot =  np.inner(word_fair_unfair,sentence_embeddings[i])\n",
    "        word_FairVecResults.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':sentences_to_test[i],\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "    word_df_fairness=pd.DataFrame(word_FairVecResults)\n",
    "    word_df_fairness['Score'] = word_df_fairness['Score'].astype(float)\n",
    "    word_df_fairness['Sentence'] = word_df_fairness['Sentence'].astype(str)\n",
    "    return word_df_fairness\n",
    "\n",
    "#Carry out word_FairVec\n",
    "word_fair_results=run_word_fairVec(fair_unfair)\n",
    "ax_word_fair = word_fair_results.plot.bar(x='Sentence', y='Score', rot=90,figsize=(14,14), fontsize=(4), legend=False )\n",
    "ax_word_fair.axvline(100.5, color='k', linestyle='--')\n",
    "plt.ylabel('Cosine Similarity', fontsize=12)\n",
    "plt.xlabel('Sentences', fontsize=10)\n",
    "plt.xticks([])\n",
    "ax_word_fair.set_facecolor(\"white\")\n",
    "plt.yticks(fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compare S-FairVec agaist the full list 200 of sentences\n",
    "\n",
    "fair_s = \"it was fair\"\n",
    "fair_sA = [fair_s]\n",
    "fair_sV = embed(fair_sA)\n",
    "\n",
    "unfair_s = \"it was unfair\"\n",
    "unfair_sA = [unfair_s]\n",
    "unfair_sV = embed(unfair_sA)\n",
    "\n",
    "word_fair_unfair= fair_sV-unfair_sV\n",
    "\n",
    "def run_word_fairVec(sentences_to_test):\n",
    "    word_FairVecResults=[]\n",
    "    lenght = len(sentences_to_test)\n",
    "    sentence_embeddings = embed(sentences_to_test)\n",
    "    for i in range(lenght):\n",
    "        dot =  np.inner(word_fair_unfair,sentence_embeddings[i])\n",
    "        word_FairVecResults.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':sentences_to_test[i],\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "    word_df_fairness=pd.DataFrame(word_FairVecResults)\n",
    "    word_df_fairness['Score'] = word_df_fairness['Score'].astype(float)\n",
    "    word_df_fairness['Sentence'] = word_df_fairness['Sentence'].astype(str)\n",
    "    return word_df_fairness\n",
    "\n",
    "#Carry out word_FairVec\n",
    "word_fair_results=run_word_fairVec(fair_unfair)\n",
    "ax_word_fair = word_fair_results.plot.bar(x='Sentence', y='Score', rot=90,figsize=(14,14), fontsize=(4), legend=False )\n",
    "ax_word_fair.axvline(100.5, color='k', linestyle='--')\n",
    "plt.ylabel('Cosine Similarity', fontsize=11)\n",
    "plt.xlabel('Sentences', fontsize=11)\n",
    "plt.xticks([])\n",
    "ax_word_fair.set_facecolor(\"white\")\n",
    "plt.yticks(fontsize=10)\n",
    "print(word_fair_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86417aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compare S-FairVec agaist a list of sentences\n",
    "\n",
    "def run_fairVec(sentences_to_test):\n",
    "    FairVecResults=[]\n",
    "    lenght = len(sentences_to_test)\n",
    "    sentence_embeddings = embed(sentences_to_test)\n",
    "   \n",
    "   # sentence_embeddings= sentence_embeddings-inappV\n",
    "    for i in range(lenght):\n",
    "        dot =  np.inner(SFairVec,sentence_embeddings[i])\n",
    "        \n",
    "        FairVecResults.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':sentences_to_test[i],\n",
    "            }\n",
    "\n",
    "        )\n",
    "  \n",
    "    df_fairness=pd.DataFrame(FairVecResults)\n",
    "    df_fairness['Score'] = df_fairness['Score'].astype(float)\n",
    "    df_fairness['Sentence'] = df_fairness['Sentence'].astype(str)\n",
    "    return df_fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1da8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out S-FairVec\n",
    "\n",
    "\n",
    "fair_results_36=run_fairVec(thirty_six_fair_unfair)\n",
    "ax_word_fair_36 = fair_results_36.plot.bar(x='Sentence', y='Score', rot=90, legend=False, figsize=(14,14))\n",
    "ax_word_fair_36.axvline(17.5, color='k', linestyle='--')\n",
    "\n",
    "plt.xticks(fontsize=15)\n",
    "plt.grid(color='0.3', linestyle='-', linewidth=0.1)\n",
    "plt.ylabel('Cosine Similarity', fontsize=16)\n",
    "plt.xlabel('Sentences', fontsize=16)\n",
    "ax_word_fair_36.grid('on', which='major', axis='x' )\n",
    "ax_word_fair_36.grid('on', which='major', axis='y' )\n",
    " \n",
    "ax_word_fair_36.set_facecolor(\"white\")\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    " \n",
    "ax_word_fair.tick_params(labelbottom=True,labeltop=False)\n",
    "plt.xticks(rotation = 90)\n",
    " \n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.grid(True, which='both')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out sentiment score of list of 36 fair unfair list and correlate\n",
    "\n",
    "def nltk_sentiment(sentence):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score\n",
    "nltk_results = [nltk_sentiment(row) for row in thirty_six_fair_unfair]\n",
    "results_df = pd.DataFrame(nltk_results)\n",
    "text_df = pd.DataFrame(thirty_six_fair_unfair, columns = ['text'])\n",
    "nltk_df = text_df.join(results_df)\n",
    "nltk_df['compound'].corr(fair_results_36['Score'])\n",
    "print(nltk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out S-FairVec on full list\n",
    "\n",
    "fair_results_200=run_fairVec(fair_unfair)\n",
    "ax_word_fair_200 = fair_results_200.plot.bar(x='Sentence', y='Score', rot=90, figsize=(14,14), fontsize=(4), legend=False )\n",
    "ax_word_fair_200.axvline(100.5, color='k', linestyle='--')\n",
    "plt.tick_params(axis='y', labelsize=11)\n",
    "plt.ylabel('Cosine Similarity', fontsize=11)\n",
    "plt.xlabel('Sentences', fontsize=11)\n",
    "plt.xticks([])\n",
    "ax_word_fair_200.set_facecolor(\"white\")\n",
    "plt.yticks(fontsize=11)\n",
    "print(fair_results_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out sentiment score of full fair unfair list and correlate \n",
    "\n",
    "nltk_results = [nltk_sentiment(row) for row in fair_unfair]\n",
    "results_df = pd.DataFrame(nltk_results)\n",
    "text_df = pd.DataFrame(fair_unfair, columns = ['text'])\n",
    "nltk_df = text_df.join(results_df)\n",
    "print(nltk_df)\n",
    "\n",
    "nltk_df['compound'].corr(fair_results_200['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_df['compound'].corr(fair_results_200['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compare Fair-Unfair vec agaist a list of complete sentences\n",
    "\n",
    "fa = \"it was fair\"# \n",
    "faE = [fa]\n",
    "faV = embed(faE)\n",
    "\n",
    "unfa = \"it was unfair\" \n",
    "unfaE = [unfa]\n",
    "unfaV = embed(unfaE)\n",
    "\n",
    "fairWordsVec= faV-unfaV\n",
    "\n",
    "def run_fairWordsVec(sentences_to_test):\n",
    "    UseVecResults=[]\n",
    "    lenght = len(sentences_to_test)\n",
    "    sentence_embeddings = embed(sentences_to_test)\n",
    " #   sentence_embeddings= sentence_embeddings-inappV\n",
    "    for i in range(lenght):\n",
    "        dot =  np.inner(fairWordsVec,sentence_embeddings[i])\n",
    "        UseVecResults.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':sentences_to_test[i],\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "    df_usefullness=pd.DataFrame(UseVecResults)\n",
    "    df_usefullness['Score'] = df_usefullness['Score'].astype(float)\n",
    "    #df_wantedness_even_if_punative['Score2'] = df_wantedness_even_if_punative['Score2'].astype(float)\n",
    "    df_usefullness['Sentence'] = df_usefullness['Sentence'].astype(str)\n",
    "    return df_usefullness#.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_results=run_fairWordsVec(fair_unfair)\n",
    "print(fw_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874375cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_results=run_fairWordsVec(fair_unfair)\n",
    "sorted_results_fw=fw_results.sort_values(by='Score', ascending=False)\n",
    "print(sorted_results_fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464331f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the individual fairness vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63532417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResponsibilityV\n",
    "DimsTableR = []\n",
    "lenght = len(thirty_six_fair_unfair)\n",
    "sentence_embeddings_test_s = embed(thirty_six_fair_unfair)\n",
    "        \n",
    "        \n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(ResponsibilityV,sentence_embeddings_test_s[i])\n",
    "    DimsTableR.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':thirty_six_fair_unfair[i],\n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "responsibility=pd.DataFrame(DimsTableR)\n",
    "#df3.sort_values(by='Score', ascending=False)\n",
    "        \n",
    "responsibility['Score'] = responsibility['Score'].astype(float)\n",
    "responsibility['Sentence'] = responsibility['Sentence'].astype(str)\n",
    "responsibilityVec=responsibility\n",
    "\n",
    "ax_res = responsibility.plot.bar(x='Sentence', y='Score', rot=90, legend=False, figsize=(14,14))\n",
    "\n",
    "ax_res.axvline(17.5, color='k', linestyle='--')\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(color='0.3', linestyle='-', linewidth=0.1)\n",
    "plt.ylabel('Cosine Similarity', fontsize=18)\n",
    "plt.xlabel('Sentences', fontsize=18)\n",
    "ax_res.set_facecolor(\"white\")\n",
    "ax_res.grid('on', which='major', axis='x' )\n",
    "ax_res.grid('on', which='major', axis='y' )\n",
    "plt.ylim(-0.25, 0.25)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_res=responsibility.sort_values(by='Score', ascending=False) \n",
    "print(result_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c303988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EmotionV\n",
    "DimsTableE = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(EmotionV,sentence_embeddings_test_s[i])\n",
    "    DimsTableE.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':thirty_six_fair_unfair[i],\n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df4=pd.DataFrame(DimsTableE)\n",
    "\n",
    "        \n",
    "df4['Score'] = df4['Score'].astype(float)\n",
    "df4['Sentence'] = df4['Sentence'].astype(str)\n",
    "emoVec=df4\n",
    "\n",
    "\n",
    "\n",
    "ax_res = df4.plot.bar(x='Sentence', y='Score', rot=90, legend=False, figsize=(14,14))\n",
    "ax_res.axvline(17.5, color='k', linestyle='--')\n",
    "plt.xticks([])\n",
    "\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(color='0.3', linestyle='-', linewidth=0.1)\n",
    "plt.ylabel('Cosine Similarity', fontsize=18)\n",
    "plt.xlabel('Sentences', fontsize=18)\n",
    "ax_res.set_facecolor(\"white\")\n",
    "ax_res.grid('on', which='major', axis='x' )\n",
    "ax_res.grid('on', which='major', axis='y' )\n",
    "plt.ylim(-0.25, 0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_emos=df4.sort_values(by='Score', ascending=False) \n",
    "print(result_emos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed90a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConsequenceV\n",
    "DimsTableC = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(ConsequenceV,sentence_embeddings_test_s[i])\n",
    "    DimsTableC.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':thirty_six_fair_unfair[i],\n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df5=pd.DataFrame(DimsTableC)\n",
    "\n",
    "        \n",
    "df5['Score'] = df5['Score'].astype(float)\n",
    "df5['Sentence'] = df5['Sentence'].astype(str)\n",
    "ConsqVec=df5\n",
    "\n",
    "\n",
    "ax_con = df5.plot.bar(x='Sentence', y='Score', rot=90, legend=False, figsize=(14,14))\n",
    "ax_con.axvline(17.5, color='k', linestyle='--')\n",
    "ax_con.set_facecolor(\"white\")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(color='0.3', linestyle='-', linewidth=0.1)\n",
    "plt.ylabel('Cosine Similarity', fontsize=18)\n",
    "plt.xlabel('Sentences', fontsize=18)\n",
    "\n",
    "\n",
    "ax_con.grid('on', which='major', axis='x' )\n",
    "ax_con.grid('on', which='major', axis='y' )\n",
    "plt.ylim(-0.25, 0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_consq=df5.sort_values(by='Score', ascending=False) \n",
    "print(result_consq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BeneficialV\n",
    "DimsTableM = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(BeneficialV,sentence_embeddings_test_s[i])\n",
    "    DimsTableM.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':thirty_six_fair_unfair[i],\n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df6=pd.DataFrame(DimsTableM)\n",
    "\n",
    "        \n",
    "df6['Score'] = df6['Score'].astype(float)\n",
    "df6['Sentence'] = df6['Sentence'].astype(str)\n",
    "BenVec=df6\n",
    "\n",
    "\n",
    "ax_ben = df6.plot.bar(x='Sentence', y='Score', rot=90, legend=False, figsize=(14,14))\n",
    "ax_ben.axvline(17.5, color='k', linestyle='--')\n",
    "ax_ben.set_facecolor(\"white\")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(color='0.3', linestyle='-', linewidth=0.1)\n",
    "plt.ylabel('Cosine Similarity', fontsize=18)\n",
    "plt.xlabel('Sentences', fontsize=18)\n",
    "\n",
    "\n",
    "ax_ben.grid('on', which='major', axis='x' )\n",
    "ax_ben.grid('on', which='major', axis='y' )\n",
    "plt.ylim(-0.25, 0.25)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d349289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HarmV\n",
    "DimsTableH = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(HarmV,sentence_embeddings_test_s[i])\n",
    "    DimsTableH.append(\n",
    "            {\n",
    "                'Score': dot,\n",
    "                'Sentence':thirty_six_fair_unfair[i],\n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df7=pd.DataFrame(DimsTableH)\n",
    "\n",
    "df7['Score'] = df7['Score'].astype(float)\n",
    "df7['Sentence'] = df7['Sentence'].astype(str)\n",
    "HarmVec=df7\n",
    "\n",
    "ax_harm = df7.plot.bar(x='Sentence', y='Score', rot=90, legend=False, figsize=(14,14))\n",
    "ax_harm.axvline(17.5, color='k', linestyle='--')\n",
    "ax_harm.set_facecolor(\"white\")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(color='0.3', linestyle='-', linewidth=0.1)\n",
    "plt.ylabel('Cosine Similarity', fontsize=18)\n",
    "plt.xlabel('Sentences', fontsize=18)\n",
    "\n",
    "\n",
    "ax_harm.grid('on', which='major', axis='x' )\n",
    "ax_harm.grid('on', which='major', axis='y' )\n",
    "plt.ylim(-0.25, 0.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=6)\n",
    "  fig, ax = plt.subplots(figsize=(100,100))  \n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\",\n",
    "      square=True, annot=True)\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "    \n",
    "def run_and_plot(messages_):\n",
    "  message_embeddings_ = embed(messages_)\n",
    "  plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa821f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test similarity score of USE for Vector phrases \n",
    "messages = [\n",
    "    \n",
    "    \"responsible\",\n",
    "   \n",
    "    \"irresponsible\",\n",
    "    \n",
    "    \"not responsible\",\n",
    "   \n",
    "   \n",
    "]\n",
    "\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "\n",
    "run_and_plot(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Dataset D1\n",
    "\n",
    "lenght = len(fair_unfair)\n",
    "sentence_embeddings_test_s = embed(fair_unfair)\n",
    "\n",
    "#ResponsibilityV\n",
    "\n",
    "DimsTableR = []\n",
    "\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(ResponsibilityV,sentence_embeddings_test_s[i])\n",
    "    DimsTableR.append(\n",
    "            {\n",
    "                'Responsibility': dot,\n",
    "              \n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "responsibility=pd.DataFrame(DimsTableR)\n",
    " \n",
    "        \n",
    "responsibility['Responsibility'] = responsibility['Responsibility'].astype(float)\n",
    "responsibilityVec=responsibility\n",
    "\n",
    "   \n",
    "\n",
    "#EmotionV\n",
    "DimsTableE = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(EmotionV,sentence_embeddings_test_s[i])\n",
    "    DimsTableE.append(\n",
    "            {\n",
    "                'Emotion': dot,\n",
    "             \n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df4=pd.DataFrame(DimsTableE)\n",
    "   \n",
    "df4['Emotion'] = df4['Emotion'].astype(float)\n",
    " \n",
    "emoVec=df4\n",
    "\n",
    "#Harm\n",
    "DimsTableH = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(HarmV,sentence_embeddings_test_s[i])\n",
    "    DimsTableH.append(\n",
    "            {\n",
    "                'Harm': dot,\n",
    "              \n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df7=pd.DataFrame(DimsTableH)\n",
    "\n",
    "df7['Harm'] = df7['Harm'].astype(float)\n",
    " \n",
    "HarmVec=df7\n",
    "\n",
    "#ConsequenceV\n",
    "DimsTableC = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(ConsequenceV,sentence_embeddings_test_s[i])\n",
    "    DimsTableC.append(\n",
    "            {\n",
    "                'Consequence': dot,\n",
    "              \n",
    "              \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df5=pd.DataFrame(DimsTableC)\n",
    "\n",
    "        \n",
    "df5['Consequence'] = df5['Consequence'].astype(float)\n",
    "ConsqVec=df5\n",
    "\n",
    "\n",
    "#BeneficialV\n",
    "DimsTableM = []\n",
    "for i in range(lenght):\n",
    "    dot =  np.inner(BeneficialV,sentence_embeddings_test_s[i])\n",
    "    DimsTableM.append(\n",
    "            {\n",
    "                'Benefit': dot,\n",
    "                         \n",
    "            }\n",
    "\n",
    "    )                      \n",
    "                                   \n",
    "df6=pd.DataFrame(DimsTableM)\n",
    "\n",
    "        \n",
    "df6['Benefit'] = df6['Benefit'].astype(float)\n",
    "BenVec=df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([BenVec, ConsqVec, HarmVec, emoVec, responsibilityVec], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a065eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the labels to the dataset \n",
    "\n",
    "df = pd.DataFrame(index=range(0, 200), columns=['Evaluation'],\n",
    "                  dtype='float')\n",
    "\n",
    "df.iloc[0:100] = 'fair'\n",
    "df.iloc[100:200] = 'unfair'\n",
    "df['Evaluation'] = df['Evaluation'].astype(str)\n",
    "print(df)\n",
    "combined_with_correct_label = pd.concat([BenVec, ConsqVec, HarmVec, emoVec, responsibilityVec, df], axis=1)\n",
    "print(combined_with_correct_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ec9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA\n",
    "x=combined_with_correct_label\n",
    "features = ['Benefit', 'Consequence','Harm','Emotion','Responsibility']\n",
    "# Separating out the features\n",
    "x = combined_with_correct_label.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = combined_with_correct_label.loc[:,['Evaluation']].values\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(combined)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf  = pd.concat([principalDf, df[['Evaluation']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('Two component PCA', fontsize = 20)\n",
    "targets = ['fair', 'unfair']\n",
    "colors = ['y', 'r']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['Evaluation'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a887b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_with_correct_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba80f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot with Hue for visualizing data \n",
    "cols = ['Benefit', 'Consequence','Harm','Emotion','Responsibility', 'Evaluation']\n",
    "pp = sns.pairplot(combined_with_correct_label[cols], hue='Evaluation', size=1.8, aspect=1.8, \n",
    "                  palette={\"unfair\": \"#FF9999\", \"fair\": \"#FFE888\"},\n",
    "                  plot_kws=dict(edgecolor=\"black\", linewidth=0.5))\n",
    "fig = pp.fig \n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Plots', fontsize=14)\n",
    "\n",
    "plt.rcParams[\"axes.labelsize\"] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 3 PCA components\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = combined_with_correct_label\n",
    "features = ['Benefit', 'Consequence','Harm','Emotion','Responsibility']\n",
    "\n",
    "pca = PCA()\n",
    "components = pca.fit_transform(df[features])\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    labels=labels,\n",
    "    dimensions=range(4),\n",
    "    color=df[\"Evaluation\"]\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 PCA\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = combined_with_correct_label\n",
    "X = df[['Benefit', 'Consequence','Harm','Emotion','Responsibility']]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=df['Evaluation'],\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081434f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Logistic Regression \n",
    "\n",
    "y=y.ravel()\n",
    "from sklearn.model_selection import train_test_split\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split( x, y, test_size=1/7.0, random_state=0)\n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(train_img)\n",
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_img, train_lbl)\n",
    "logisticRegr.predict(test_img[0].reshape(1,-1))\n",
    "logisticRegr.predict(test_img[0:10])\n",
    "logisticRegr.score(test_img, test_lbl)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281057d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#CV scores, 5 fold CV\n",
    "scores = cross_val_score(logisticRegr, train_img, train_lbl, cv=5)\n",
    "\n",
    "#Prediction and accuracy\n",
    "y_pred = logisticRegr.predict(test_img)\n",
    "accuracy_test = accuracy_score(test_lbl, y_pred)\n",
    "\n",
    "#Print the summary\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print (\"Test Accuracy: %0.2f\" % (accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207407fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "score = logisticRegr.score(test_img, test_lbl)\n",
    "print (\"Accuracy: %0.2f\" % (score))\n",
    "\n",
    "#Prediction\n",
    "predictions = logisticRegr.predict(test_img)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "#confusion matrix\n",
    "cm = metrics.confusion_matrix(test_lbl, predictions)\n",
    "print(\"Confusion Matrix: \\n\", cm)\n",
    "\n",
    "#Visualize the confusion matrix\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual Class');\n",
    "plt.xlabel('Predicted Class');\n",
    "all_sample_title = \"Test Accuracy: %0.2f\" % (score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b81ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "#CV scores, 5 fold CV\n",
    "scores = cross_val_score(logisticRegr, train_img, train_lbl, cv=5)\n",
    "\n",
    "#Prediction and accuracy\n",
    "y_pred = logisticRegr.predict(test_img)\n",
    "accuracy_test = accuracy_score(test_lbl, y_pred)\n",
    "\n",
    "print(classification_report(test_lbl, y_pred))#Checking performance our model with ROC Score.\n",
    "#roc_auc_score(test_lbl, y_pred)\n",
    "\n",
    "#f1=f1_score(test_lbl, y_pred)\n",
    "#print(precision_score(test_lbl, y_pred))\n",
    "#print(recall_score(test_lbl, y_pred))\n",
    "\n",
    "#Print the summary\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print (\"Test Accuracy: %0.2f\" % (accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SBERT experiments\n",
    "from fair_unfair import fair_unfair\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_embeddings = model.encode(fair_unfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b719ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Sentence FairVec (S-FairVec) using 10 abstract concepts to represent the social ontology of fairness \n",
    "\n",
    "Res = [\"is was very responsible\"]\n",
    "Ires = [\"is was very irresponsible\"]\n",
    "joy = [\"it was joyous\"]\n",
    "pain = [\"it was sad\"]\n",
    "ben = [\"it was Benefit to society\"]\n",
    "har = [\"it was not Benefit to society\"]\n",
    "lib = [\"was free to and rewarded\"]\n",
    "pri = [\"was sent to prison and punished\"]\n",
    "app = [\"it was Benefit\"]\n",
    "inapp = [\"it was harmful\"]\n",
    "\n",
    "\n",
    "EmbedRse = model.encode(Res)\n",
    "EmbedIres = model.encode(Ires)\n",
    "Ejoy=model.encode(joy)\n",
    "Epain=model.encode(pain)\n",
    "Eben=model.encode(ben)\n",
    "Ehar=model.encode(har)\n",
    "Elib=model.encode(lib)\n",
    "Epri=model.encode(pri)\n",
    "Eapp=model.encode(app)\n",
    "Einapp=model.encode(inapp)\n",
    "\n",
    "\n",
    "BERTResponsibilityV = EmbedRse-EmbedIres\n",
    "BERTEmotionV = Ejoy - Epain \n",
    "BERTConsequenceV = Eben - Ehar   \n",
    "BERTBenefitV = Elib - Epri \n",
    "BERTHarmV = Eapp - Einapp  \n",
    "\n",
    "#This is S-FairVec which combines the above vectors into one (it needs error minimisation)\n",
    "SBERTFairVec = BERTResponsibilityV+BERTEmotionV+BERTConsequenceV+BERTBenefitV+BERTHarmV\n",
    "\n",
    "results=cosine_similarity(SBERTFairVec, sentence_embeddings[0:])\n",
    "df_results=pd.DataFrame(results)\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place results into a dataset for training \n",
    "\n",
    "\n",
    "res = cosine_similarity(BERTResponsibilityV, sentence_embeddings[0:])\n",
    "responsibility=pd.DataFrame(res)        \n",
    "\n",
    "emo = cosine_similarity(BERTEmotionV, sentence_embeddings[0:])\n",
    "emotion = pd.DataFrame(emo)\n",
    "\n",
    "hm = cosine_similarity(BERTHarmV, sentence_embeddings[0:])\n",
    "harm = pd.DataFrame(hm)\n",
    "\n",
    "con = cosine_similarity(BERTConsequenceV, sentence_embeddings[0:])\n",
    "consequence=pd.DataFrame(con)\n",
    "\n",
    "ben = cosine_similarity(BERTBenefitV, sentence_embeddings[0:])\n",
    "Benefit = pd.DataFrame(ben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=responsibility.T\n",
    "e=emotion.T\n",
    "h=harm.T\n",
    "c=consequence.T\n",
    "b=Benefit.T\n",
    "\n",
    "r.reset_index(drop=True, inplace=True)\n",
    "e.reset_index(drop=True, inplace=True)\n",
    "h.reset_index(drop=True, inplace=True)\n",
    "c.reset_index(drop=True, inplace=True)\n",
    "b.reset_index(drop=True, inplace=True)\n",
    "\n",
    "r.rename(columns={0: \"Responsibility\"}, inplace=True)\n",
    "e.rename(columns={0: \"Emotion\"}, inplace=True)\n",
    "h.rename(columns={0: \"Harm\"}, inplace=True)\n",
    "c.rename(columns={0: \"Consequence\"}, inplace=True)\n",
    "b.rename(columns={0: \"Benefit\"}, inplace=True)\n",
    "\n",
    "dataset1 = pd.concat([b, c, h, e, r], axis=1)\n",
    "display(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ea7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=range(0, 200), columns=['Evaluation'],\n",
    "                  dtype='float')\n",
    "\n",
    "df.iloc[0:100] = 'fair'\n",
    "df.iloc[100:200] = 'unfair'\n",
    "df['Evaluation'] = df['Evaluation'].astype(str)\n",
    "dataset1_with_labels = pd.concat([b, c, h, e, r, df], axis=1)\n",
    "print(dataset1_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Benefit', 'Consequence','Harm','Emotion','Responsibility']\n",
    "# Separating out the features\n",
    "x = dataset1_with_labels.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = dataset1_with_labels.loc[:,['Evaluation']].values\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(dataset1)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "finalDf  = pd.concat([principalDf, df[['Evaluation']]], axis = 1)\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('Two component PCA', fontsize = 20)\n",
    "targets = ['fair', 'unfair']\n",
    "colors = ['y', 'r']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['Evaluation'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def56f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot with Hue for visualising data \n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "sns.set(font_scale=1.5)\n",
    "cols = ['Benefit', 'Consequence','Harm','Emotion','Responsibility', 'Evaluation']\n",
    "pp = sns.pairplot(dataset1_with_labels[cols], hue='Evaluation', height=1.8, aspect=1.8, \n",
    "                  palette={\"unfair\": \"#FF9999\", \"fair\": \"#FFE888\"},\n",
    "                  plot_kws=dict(edgecolor=\"black\", linewidth=0.5))\n",
    "fig = pp.fig \n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6847fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 3 PCA components\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = dataset1_with_labels\n",
    "features = ['Benefit', 'Consequence','Harm','Emotion','Responsibility']\n",
    "\n",
    "pca = PCA()\n",
    "components = pca.fit_transform(df[features])\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    labels=labels,\n",
    "    dimensions=range(4),\n",
    "    color=df[\"Evaluation\"]\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()\n",
    "# 3 PCA\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = dataset1_with_labels\n",
    "X = df[['Benefit', 'Consequence','Harm','Emotion','Responsibility']]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=df['Evaluation'],\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()\n",
    "y=y.ravel()\n",
    "from sklearn.model_selection import train_test_split\n",
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split( x, y, test_size=1/7.0, random_state=0)\n",
    " \n",
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11892d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform logistic regresssion\n",
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_img, train_lbl)\n",
    "logisticRegr.predict(test_img[0].reshape(1,-1))\n",
    "logisticRegr.predict(test_img[0:10])\n",
    "logisticRegr.score(test_img, test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "#CV scores, 5 fold CV\n",
    "scores = cross_val_score(logisticRegr, train_img, train_lbl, cv=5)\n",
    "\n",
    "#Prediction and accuracy\n",
    "y_pred = logisticRegr.predict(test_img)\n",
    "accuracy_test = accuracy_score(test_lbl, y_pred)\n",
    "\n",
    "print(classification_report(test_lbl, y_pred))#Checking performance our model with ROC Score.\n",
    "#roc_auc_score(test_lbl, y_pred)\n",
    "\n",
    "#f1=f1_score(test_lbl, y_pred)\n",
    "#print(precision_score(test_lbl, y_pred))\n",
    "#print(recall_score(test_lbl, y_pred))\n",
    "\n",
    "#Print the summary\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print (\"Test Accuracy: %0.2f\" % (accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d004b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "score = logisticRegr.score(test_img, test_lbl)\n",
    "print (\"Accuracy: %0.2f\" % (score))\n",
    "\n",
    "#Prediction\n",
    "predictions = logisticRegr.predict(test_img)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RoBERTa results \n",
    "#Uses code from Cardiff NLP MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"The jury convicted the innocent\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63886b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "\n",
    "roberta_list1=[]\n",
    "\n",
    "\n",
    "for j in range (199):\n",
    "    text = fair_unfair[j]\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        #print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "        roberta_list1.append(\n",
    "            {\n",
    "                'Sentence':fair_unfair[j],\n",
    "                'l score': l,\n",
    "                's score': s,\n",
    "            }\n",
    "        )  \n",
    "roberta_data=pd.DataFrame(roberta_list1)\n",
    "display(roberta_data)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roberta_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0982e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fair_unfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_data.to_csv(\"SVM_Data.csv\", encoding='utf-8', index=False)\n",
    "top_roberta_answer=roberta_data.iloc[::3, :]\n",
    "display(top_roberta_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fcf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_roberta_answer['l score'] = top_roberta_answer['l score'].str.replace('Positive','+')\n",
    "top_roberta_answer['l score'] = top_roberta_answer['l score'].str.replace('Neutral','+')\n",
    "top_roberta_answer['l score'] = top_roberta_answer['l score'].str.replace('Negative','-')\n",
    "display(top_roberta_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7eeb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_roberta_answer['new score'] = top_roberta_answer['l score'] + top_roberta_answer['s score'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_roberta_answer.dtypes\n",
    "top_roberta_answer['new score'] = pd.to_numeric(top_roberta_answer['new score'])\n",
    "display(top_roberta_answer)\n",
    "top_roberta_answer['new score']. round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Earlier Fairness vector results from the 200 sentences read \n",
    "fairvecresults = pd.read_csv ('fairvec.csv')\n",
    "print(fairvecresults)\n",
    "\n",
    "top_roberta_answer.reset_index(drop=True)\n",
    "display(top_roberta_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51528639",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_roberta_answer.reset_index(drop=True)\n",
    "display(top_roberta_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_roberta_answer.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b62dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_roberta_answer.reset_index(drop=True, inplace=True)\n",
    "display(top_roberta_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf25a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now perform the correlation of the fariness vector results with the RoBERTa sentiment analyser results found above\n",
    "\n",
    "fairvecresults['FairVec'].corr(top_roberta_answer['new score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
